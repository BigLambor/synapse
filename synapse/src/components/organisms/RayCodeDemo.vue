<template>
  <AppCard variant="elevated">
    <template #header>
      <div class="flex items-center justify-between">
        <div>
          <h2 class="text-xl font-semibold mb-1">âš¡ Rayåˆ†å¸ƒå¼å¤„ç†å®ç°</h2>
          <p class="text-sm text-neutral-400">é«˜æ€§èƒ½å¹¶è¡Œç‰¹å¾æå–ä»£ç ç¤ºä¾‹</p>
        </div>
        <div class="flex gap-2">
          <AppButton
            v-for="example in codeExamples"
            :key="example.id"
            size="sm"
            :variant="selectedExample === example.id ? 'primary' : 'ghost'"
            @click="selectedExample = example.id"
          >
            {{ example.label }}
          </AppButton>
        </div>
      </div>
    </template>

    <div class="space-y-4">
      <!-- ä»£ç å±•ç¤º -->
      <div class="relative">
        <div class="absolute top-3 right-3 z-10">
          <AppButton size="sm" variant="ghost" @click="copyCode">
            ğŸ“‹ å¤åˆ¶ä»£ç 
          </AppButton>
        </div>
        <pre class="bg-neutral-950 rounded-lg p-4 overflow-x-auto border border-neutral-800"><code class="text-xs text-neutral-300 font-mono leading-relaxed">{{ currentCode }}</code></pre>
      </div>

      <!-- ä»£ç è¯´æ˜ -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
        <div class="bg-gradient-to-br from-primary-500/10 to-primary-600/10 rounded-lg p-4 border border-primary-500/20">
          <h4 class="font-semibold text-sm mb-2 flex items-center gap-2">
            <span>ğŸ¯</span>
            <span>æ ¸å¿ƒç‰¹æ€§</span>
          </h4>
          <ul class="space-y-2 text-xs text-neutral-300">
            <li v-for="feature in currentExample.features" :key="feature" class="flex items-start gap-2">
              <span class="text-primary-400 mt-0.5">â€¢</span>
              <span>{{ feature }}</span>
            </li>
          </ul>
        </div>

        <div class="bg-gradient-to-br from-secondary-500/10 to-secondary-600/10 rounded-lg p-4 border border-secondary-500/20">
          <h4 class="font-semibold text-sm mb-2 flex items-center gap-2">
            <span>ğŸ“ˆ</span>
            <span>æ€§èƒ½ä¼˜åŠ¿</span>
          </h4>
          <ul class="space-y-2 text-xs text-neutral-300">
            <li v-for="benefit in currentExample.benefits" :key="benefit" class="flex items-start gap-2">
              <span class="text-secondary-400 mt-0.5">â€¢</span>
              <span>{{ benefit }}</span>
            </li>
          </ul>
        </div>
      </div>

      <!-- æ‰§è¡Œæµç¨‹å¯è§†åŒ– -->
      <div class="bg-neutral-900/50 rounded-lg p-4 border border-neutral-800">
        <h4 class="font-semibold text-sm mb-4 flex items-center gap-2">
          <span>ğŸ”„</span>
          <span>æ‰§è¡Œæµç¨‹</span>
        </h4>
        <div class="space-y-3">
          <div
            v-for="(step, index) in currentExample.workflow"
            :key="index"
            class="flex items-start gap-3"
          >
            <div class="flex-shrink-0 w-6 h-6 rounded-full bg-primary-500/20 text-primary-400 flex items-center justify-center text-xs font-semibold">
              {{ index + 1 }}
            </div>
            <div class="flex-1">
              <div class="font-medium text-sm mb-1">{{ step.title }}</div>
              <div class="text-xs text-neutral-400">{{ step.description }}</div>
            </div>
            <div v-if="step.time" class="text-xs text-neutral-500 whitespace-nowrap">
              {{ step.time }}
            </div>
          </div>
        </div>
      </div>

      <!-- æ€§èƒ½å¯¹æ¯” -->
      <div class="bg-gradient-to-r from-green-500/10 to-blue-500/10 rounded-lg p-4 border border-green-500/20">
        <h4 class="font-semibold text-sm mb-3 flex items-center gap-2">
          <span>âš¡</span>
          <span>æ€§èƒ½å¯¹æ¯”</span>
        </h4>
        <div class="grid grid-cols-3 gap-4">
          <div class="text-center">
            <div class="text-2xl font-bold text-red-400 mb-1">{{ currentExample.performance.sequential }}</div>
            <div class="text-xs text-neutral-400">å•çº¿ç¨‹å¤„ç†</div>
          </div>
          <div class="text-center">
            <div class="text-2xl font-bold text-yellow-400 mb-1">{{ currentExample.performance.multiprocess }}</div>
            <div class="text-xs text-neutral-400">å¤šè¿›ç¨‹å¤„ç†</div>
          </div>
          <div class="text-center">
            <div class="text-2xl font-bold text-green-400 mb-1">{{ currentExample.performance.ray }}</div>
            <div class="text-xs text-neutral-400">Rayåˆ†å¸ƒå¼</div>
          </div>
        </div>
        <div class="mt-3 text-center">
          <AppBadge variant="success">
            æ€§èƒ½æå‡ {{ currentExample.performance.improvement }}
          </AppBadge>
        </div>
      </div>
    </div>
  </AppCard>
</template>

<script setup lang="ts">
import { ref, computed } from 'vue'
import AppCard from '@/components/atoms/AppCard.vue'
import AppButton from '@/components/atoms/AppButton.vue'
import AppBadge from '@/components/atoms/AppBadge.vue'

const codeExamples = [
  { id: 'basic', label: 'åŸºç¡€æ¶æ„' },
  { id: 'document', label: 'æ–‡æ¡£å¤„ç†' },
  { id: 'multimodal', label: 'å¤šæ¨¡æ€' }
]

const selectedExample = ref('basic')

const examplesData = {
  basic: {
    code: `import ray
from transformers import AutoTokenizer, AutoModel
import torch

# åˆå§‹åŒ–Rayé›†ç¾¤
ray.init(address='auto', ignore_reinit_error=True)

@ray.remote(num_gpus=0.25)  # æ¯ä¸ªä»»åŠ¡åˆ†é…0.25ä¸ªGPU
class FeatureExtractor:
    """åˆ†å¸ƒå¼ç‰¹å¾æå–å™¨"""
    
    def __init__(self, model_name: str = "bert-base-chinese"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
    
    def extract_features(self, text: str) -> dict:
        """æå–æ–‡æœ¬ç‰¹å¾å‘é‡"""
        with torch.no_grad():
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                max_length=512, 
                truncation=True,
                padding=True
            ).to(self.device)
            
            outputs = self.model(**inputs)
            # ä½¿ç”¨[CLS] tokençš„è¾“å‡ºä½œä¸ºå¥å­è¡¨ç¤º
            features = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
        return {
            "vector": features.tolist()[0],  # 768ç»´å‘é‡
            "dimension": features.shape[1],
            "model": self.model.config.name_or_path
        }

# åˆ›å»º4ä¸ªåˆ†å¸ƒå¼ç‰¹å¾æå–å™¨å®ä¾‹
extractors = [FeatureExtractor.remote() for _ in range(4)]

# å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£
documents = [
    "AIæŠ€æœ¯åœ¨æ•°æ®å¤„ç†ä¸­çš„åº”ç”¨...",
    "å¤šæ¨¡æ€ç‰¹å¾æå–çš„æœ€ä½³å®è·µ...",
    "å‘é‡æ•°æ®åº“çš„é€‰å‹ä¸ä¼˜åŒ–...",
    # ... æ›´å¤šæ–‡æ¡£
]

# åˆ†å‘ä»»åŠ¡åˆ°ä¸åŒçš„worker
futures = []
for i, doc in enumerate(documents):
    extractor = extractors[i % len(extractors)]  # è½®è¯¢åˆ†é…
    future = extractor.extract_features.remote(doc)
    futures.append(future)

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
results = ray.get(futures)
print(f"å¤„ç†å®Œæˆ {len(results)} ä¸ªæ–‡æ¡£")`,
    features: [
      '@ray.remoteè£…é¥°å™¨å°†ç±»è½¬æ¢ä¸ºåˆ†å¸ƒå¼Actor',
      'è‡ªåŠ¨GPUèµ„æºåˆ†é…å’Œç®¡ç†',
      'æ”¯æŒå¤šå®ä¾‹å¹¶è¡Œå¤„ç†',
      'å¼‚æ­¥ä»»åŠ¡æäº¤ï¼Œéé˜»å¡æ‰§è¡Œ'
    ],
    benefits: [
      '4ä¸ªGPUå¹¶è¡Œï¼Œå¤„ç†é€Ÿåº¦æå‡3-4å€',
      'è‡ªåŠ¨è´Ÿè½½å‡è¡¡ï¼Œå……åˆ†åˆ©ç”¨èµ„æº',
      'å®¹é”™æœºåˆ¶ï¼Œä»»åŠ¡å¤±è´¥è‡ªåŠ¨é‡è¯•',
      'æ˜“äºæ‰©å±•åˆ°æ›´å¤šæœºå™¨'
    ],
    workflow: [
      { title: 'åˆå§‹åŒ–Rayé›†ç¾¤', description: 'è¿æ¥åˆ°Rayé›†ç¾¤ï¼Œå‘ç°å¯ç”¨èµ„æºï¼ˆCPUã€GPUã€å†…å­˜ï¼‰', time: '0.5s' },
      { title: 'åˆ›å»ºActorå®ä¾‹', description: 'åœ¨ä¸åŒèŠ‚ç‚¹ä¸Šå¯åŠ¨4ä¸ªFeatureExtractorå®ä¾‹ï¼ŒåŠ è½½æ¨¡å‹', time: '5-10s' },
      { title: 'ä»»åŠ¡åˆ†å‘', description: 'å°†æ–‡æ¡£è½®è¯¢åˆ†é…ç»™ä¸åŒçš„Actorï¼Œå®ç°è´Ÿè½½å‡è¡¡', time: '~0.1s' },
      { title: 'å¹¶è¡Œæ‰§è¡Œ', description: 'æ‰€æœ‰ActoråŒæ—¶å¤„ç†å„è‡ªçš„æ–‡æ¡£ï¼Œäº’ä¸å¹²æ‰°', time: '2-5s' },
      { title: 'ç»“æœæ”¶é›†', description: 'ray.get()ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ”¶é›†ç»“æœ', time: '~0.1s' }
    ],
    performance: {
      sequential: '120s',
      multiprocess: '35s',
      ray: '15s',
      improvement: '8å€'
    }
  },
  document: {
    code: `import ray
from typing import List, Dict
import fitz  # PyMuPDF
from transformers import pipeline

@ray.remote
class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ - æå–æ–‡æœ¬å¹¶ç”Ÿæˆç‰¹å¾"""
    
    def __init__(self):
        # åŠ è½½NERæ¨¡å‹ç”¨äºå®ä½“è¯†åˆ«
        self.ner = pipeline("ner", model="ckiplab/bert-base-chinese-ner")
        # åŠ è½½ç‰¹å¾æå–æ¨¡å‹
        self.feature_extractor = pipeline(
            "feature-extraction",
            model="bert-base-chinese"
        )
    
    def process_pdf(self, file_path: str, asset_id: str) -> Dict:
        """å¤„ç†PDFæ–‡æ¡£"""
        # 1. æå–æ–‡æœ¬
        doc = fitz.open(file_path)
        full_text = ""
        page_contents = []
        
        for page_num, page in enumerate(doc):
            text = page.get_text()
            full_text += text
            page_contents.append({
                "page": page_num + 1,
                "text": text[:500]  # ä¿å­˜å‰500å­—ç¬¦
            })
        
        # 2. æå–ç‰¹å¾å‘é‡
        features = self.feature_extractor(
            full_text[:512],  # å–å‰512ä¸ªtoken
            return_tensors=False
        )
        # å¯¹æ‰€æœ‰tokençš„å‘é‡å–å¹³å‡
        vector = [sum(x)/len(x) for x in zip(*features[0])]
        
        # 3. å®ä½“è¯†åˆ«
        entities = self.ner(full_text[:1000])
        entity_summary = self._aggregate_entities(entities)
        
        # 4. ç”Ÿæˆå…ƒæ•°æ®
        metadata = {
            "asset_id": asset_id,
            "type": "document",
            "pages": len(doc),
            "total_chars": len(full_text),
            "page_contents": page_contents
        }
        
        return {
            "asset_id": asset_id,
            "vector": vector,  # 768ç»´
            "entities": entity_summary,
            "metadata": metadata,
            "status": "completed"
        }
    
    def _aggregate_entities(self, entities: List) -> List[Dict]:
        """èšåˆå®ä½“è¯†åˆ«ç»“æœ"""
        entity_counts = {}
        for ent in entities:
            name = ent['word']
            ent_type = ent['entity']
            key = f"{name}_{ent_type}"
            entity_counts[key] = entity_counts.get(key, 0) + 1
        
        return [
            {
                "name": k.split('_')[0],
                "type": k.split('_')[1],
                "count": v
            }
            for k, v in sorted(
                entity_counts.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:10]  # è¿”å›top 10
        ]

# ä½¿ç”¨ç¤ºä¾‹
@ray.remote
def batch_process_documents(file_paths: List[str]) -> List[Dict]:
    """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
    processor = DocumentProcessor.remote()
    
    futures = []
    for i, path in enumerate(file_paths):
        asset_id = f"doc_{i:06d}"
        future = processor.process_pdf.remote(path, asset_id)
        futures.append(future)
    
    results = ray.get(futures)
    return results

# å¤„ç†1000ä¸ªæ–‡æ¡£
document_paths = get_all_documents()  # è·å–æ‰€æœ‰æ–‡æ¡£è·¯å¾„
results = ray.get(batch_process_documents.remote(document_paths))`,
    features: [
      'PDFæ–‡æœ¬æå– + ç‰¹å¾å‘é‡ç”Ÿæˆ',
      'NERå®ä½“è¯†åˆ«ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯',
      'åˆ†é¡µå†…å®¹ä¿å­˜ï¼Œæ”¯æŒç²¾ç¡®å®šä½',
      'å…ƒæ•°æ®å®Œæ•´è®°å½•ï¼Œä¾¿äºæ£€ç´¢è¿‡æ»¤'
    ],
    benefits: [
      'å¹¶è¡Œå¤„ç†ï¼Œ1000ä¸ªæ–‡æ¡£<5åˆ†é’Ÿ',
      'ç‰¹å¾+å…ƒæ•°æ®åŒé‡ç´¢å¼•',
      'å®ä½“å…³è”ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±',
      'åŸå§‹å†…å®¹ä¿ç•™ï¼Œæ”¯æŒæº¯æº'
    ],
    workflow: [
      { title: 'PDFè§£æ', description: 'ä½¿ç”¨PyMuPDFæå–æ¯é¡µæ–‡æœ¬å†…å®¹', time: '0.5s/doc' },
      { title: 'ç‰¹å¾æå–', description: 'BERTç¼–ç ç”Ÿæˆ768ç»´å‘é‡è¡¨ç¤º', time: '2s/doc' },
      { title: 'å®ä½“è¯†åˆ«', description: 'NERæ¨¡å‹è¯†åˆ«äººåã€åœ°åã€æœºæ„ç­‰å®ä½“', time: '1s/doc' },
      { title: 'å…ƒæ•°æ®ç”Ÿæˆ', description: 'æ•´ç†é¡µæ•°ã€å­—æ•°ã€åˆ†é¡µå†…å®¹ç­‰ä¿¡æ¯', time: '0.1s/doc' },
      { title: 'ç»“æœè¿”å›', description: 'åŒ…å«å‘é‡ã€å®ä½“ã€å…ƒæ•°æ®çš„å®Œæ•´ç»“æœ', time: '~0s' }
    ],
    performance: {
      sequential: '3500s',
      multiprocess: '450s',
      ray: '300s',
      improvement: '12å€'
    }
  },
  multimodal: {
    code: `import ray
from typing import Dict, List, Union
import torch
from transformers import CLIPProcessor, CLIPModel
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import cv2
import librosa

@ray.remote(num_gpus=0.5)
class MultiModalFeatureExtractor:
    """å¤šæ¨¡æ€ç‰¹å¾æå–å™¨ - ç»Ÿä¸€çš„ç‰¹å¾ç©ºé—´"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # CLIP: å›¾åƒå’Œæ–‡æœ¬çš„è”åˆç¼–ç 
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_model.to(self.device)
        
        # Wav2Vec2: éŸ³é¢‘ç‰¹å¾æå–
        self.audio_processor = Wav2Vec2Processor.from_pretrained(
            "facebook/wav2vec2-base"
        )
        self.audio_model = Wav2Vec2Model.from_pretrained(
            "facebook/wav2vec2-base"
        )
        self.audio_model.to(self.device)
    
    def extract_image_features(self, image_path: str) -> Dict:
        """æå–å›¾åƒç‰¹å¾"""
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        with torch.no_grad():
            inputs = self.clip_processor(
                images=image, 
                return_tensors="pt"
            ).to(self.device)
            features = self.clip_model.get_image_features(**inputs)
            vector = features.cpu().numpy()[0]
        
        return {
            "vector": vector.tolist(),
            "dimension": len(vector),
            "modality": "image"
        }
    
    def extract_audio_features(self, audio_path: str) -> Dict:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=16000)
        
        with torch.no_grad():
            inputs = self.audio_processor(
                audio, 
                sampling_rate=16000, 
                return_tensors="pt"
            ).to(self.device)
            outputs = self.audio_model(**inputs)
            # å¯¹æ—¶é—´ç»´åº¦å–å¹³å‡
            vector = outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]
        
        return {
            "vector": vector.tolist(),
            "dimension": len(vector),
            "modality": "audio"
        }
    
    def extract_video_features(self, video_path: str) -> Dict:
        """æå–è§†é¢‘ç‰¹å¾ - å…³é”®å¸§é‡‡æ ·"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # æ¯ç§’é‡‡æ ·1å¸§
        sample_interval = int(fps)
        frame_features = []
        
        for frame_idx in range(0, total_frames, sample_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if not ret:
                break
            
            # æå–å¸§ç‰¹å¾
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            with torch.no_grad():
                inputs = self.clip_processor(
                    images=frame, 
                    return_tensors="pt"
                ).to(self.device)
                features = self.clip_model.get_image_features(**inputs)
                frame_features.append(features.cpu().numpy()[0])
        
        cap.release()
        
        # å¯¹æ‰€æœ‰å¸§ç‰¹å¾å–å¹³å‡
        avg_vector = sum(frame_features) / len(frame_features)
        
        return {
            "vector": avg_vector.tolist(),
            "dimension": len(avg_vector),
            "modality": "video",
            "sampled_frames": len(frame_features)
        }

@ray.remote
def process_multimodal_batch(assets: List[Dict]) -> List[Dict]:
    """æ‰¹é‡å¤„ç†å¤šæ¨¡æ€èµ„äº§"""
    extractor = MultiModalFeatureExtractor.remote()
    
    futures = []
    for asset in assets:
        if asset['type'] == 'image':
            future = extractor.extract_image_features.remote(asset['path'])
        elif asset['type'] == 'audio':
            future = extractor.extract_audio_features.remote(asset['path'])
        elif asset['type'] == 'video':
            future = extractor.extract_video_features.remote(asset['path'])
        
        futures.append((asset['id'], future))
    
    results = []
    for asset_id, future in futures:
        feature_data = ray.get(future)
        results.append({
            "asset_id": asset_id,
            **feature_data
        })
    
    return results

# æ··åˆæ‰¹é‡å¤„ç†
mixed_assets = [
    {"id": "img_001", "type": "image", "path": "/data/images/001.jpg"},
    {"id": "aud_001", "type": "audio", "path": "/data/audio/001.mp3"},
    {"id": "vid_001", "type": "video", "path": "/data/videos/001.mp4"},
    # ... æ›´å¤šèµ„äº§
]

results = ray.get(process_multimodal_batch.remote(mixed_assets))`,
    features: [
      'CLIPæ¨¡å‹å®ç°å›¾åƒ-æ–‡æœ¬è”åˆç¼–ç ',
      'Wav2Vec2ç›´æ¥ä»éŸ³é¢‘æ³¢å½¢å­¦ä¹ è¡¨ç¤º',
      'è§†é¢‘å…³é”®å¸§é‡‡æ ·ï¼Œå¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡',
      'ç»Ÿä¸€çš„ç‰¹å¾ç©ºé—´ï¼Œæ”¯æŒè·¨æ¨¡æ€æ£€ç´¢'
    ],
    benefits: [
      'è·¨æ¨¡æ€æœç´¢ï¼šç”¨æ–‡å­—æœå›¾ç‰‡/è§†é¢‘',
      'ç‰¹å¾ç©ºé—´å¯¹é½ï¼Œç›¸ä¼¼åº¦å¯æ¯”è¾ƒ',
      'GPUåŠ é€Ÿï¼Œå¤„ç†é€Ÿåº¦æå‡20-50å€',
      'å¯æ‰©å±•æ¶æ„ï¼Œæ˜“äºæ·»åŠ æ–°æ¨¡æ€'
    ],
    workflow: [
      { title: 'èµ„äº§åˆ†ç±»', description: 'æ ¹æ®æ–‡ä»¶ç±»å‹è·¯ç”±åˆ°ä¸åŒçš„å¤„ç†å™¨', time: '~0s' },
      { title: 'æ¨¡æ€ç‰¹å®šå¤„ç†', description: 'å›¾ç‰‡ç”¨CLIPã€éŸ³é¢‘ç”¨Wav2Vec2ã€è§†é¢‘é‡‡æ ·å…³é”®å¸§', time: '1-5s' },
      { title: 'ç‰¹å¾å½’ä¸€åŒ–', description: 'å°†ä¸åŒæ¨¡æ€çš„ç‰¹å¾æŠ•å½±åˆ°ç»Ÿä¸€ç©ºé—´', time: '0.1s' },
      { title: 'è´¨é‡æ£€æŸ¥', description: 'éªŒè¯ç‰¹å¾ç»´åº¦ã€æ•°å€¼èŒƒå›´æ˜¯å¦æ­£å¸¸', time: '0.1s' },
      { title: 'æ‰¹é‡è¿”å›', description: 'åŒ…å«asset_idã€å‘é‡ã€æ¨¡æ€ç±»å‹çš„ç»“æœ', time: '~0s' }
    ],
    performance: {
      sequential: '5400s',
      multiprocess: '720s',
      ray: '180s',
      improvement: '30å€'
    }
  }
}

const currentExample = computed(() => examplesData[selectedExample.value as keyof typeof examplesData])
const currentCode = computed(() => currentExample.value.code)

const copyCode = () => {
  navigator.clipboard.writeText(currentCode.value)
  console.log('ä»£ç å·²å¤åˆ¶åˆ°å‰ªè´´æ¿')
}
</script>

